// Copyright 2017-2018 The Khronos Group. This work is licensed under a
// Creative Commons Attribution 4.0 International License; see
// http://creativecommons.org/licenses/by/4.0/

[[cl_khr_fp64]]
== Double Precision Floating-Point

This section describes the *cl_khr_fp64* extension. 
This extension became an optional core feature in OpenCL 1.2.

[[cl_khr_fp64-additions-to-chapter-6]]
=== Additions to Chapter 6

The list of built-in scalar, and vector data types defined in _tables 6.1_
and _6.2_ are extended to include the following:

[cols="1,3",options="header",]
|====
|*Type*     | *Description*
|*double*   | A double precision float.
|*double2*  | A 2-component double-precision floating-point vector.
|*double3*  | A 3-component double-precision floating-point vector.
|*double4*  | A 4-component double-precision floating-point vector.
|*double8*  | A 8-component double-precision floating-point vector.
|*double16* | A 16-component double-precision floating-point vector.
|====

The built-in scalar and vector data types for `doublen` are also declared as appropriate
types in the OpenCL API (and header files) that can be used by an
application.
The following table describes the built-in scalar and vector data types for `doublen` as
defined in the OpenCL C programming language and the corresponding data type
available to the application:

[cols=",",options="header",]
|====
|*Type in OpenCL Language* | *API type for application*
|*double*                  | *cl_double*
|*double2*                 | *cl_double2*
|*double3*                 | *cl_double3*
|*double4*                 | *cl_double4*
|*double8*                 | *cl_double8*
|*double16*                | *cl_double16*
|====

The double data type must conform to the IEEE-754 double precision storage format.

The following text is added to _Section 6.1.1.1 The half data type_:

Conversions from double to half are correctly rounded.
Conversions from half to double are lossless.

[[cl_khr_fp64-conversions]]
==== Conversions

The implicit conversion rules specified in _section 6.2.1_ now include the
`double` scalar and `doublen` vector data types.

The explicit casts described in _section 6.2.2_ are extended to take a
`double` scalar data type and a `doublen` vector data type.

The explicit conversion functions described in _section 6.2.3_ are extended
to take a `double` scalar data type and a `doublen` vector data type.

The `as_typen()` function for re-interpreting types as described in _section
6.2.4.2_ is extended to allow conversion-free casts between `longn`,
`ulongn` and `doublen` scalar and vector data types. 

[[cl_khr_fp64-math-functions]]
==== Math Functions

The built-in math functions defined in _table 6.8_ (also listed below) are
extended to include appropriate versions of functions that take `double` and
`double{2|3|4|8|16}` as arguments and return values.
`gentype` now also includes `double`, `double2`, `double3`, `double4`, `double8` and
`double16`.

For any specific use of a function, the actual type has to be the same for
all arguments and the return type.

._Double Precision Built-in Math Functions_
[cols=",",options="header",]
|====
| *Function*
| *Description*

| gentype *acos* (gentype _x_)
| Arc cosine function.

| gentype *acosh* (gentype _x_)
| Inverse hyperbolic cosine.

| gentype *acospi* (gentype _x_)
| Compute *acos* (_x_) / {pi}.

| gentype *asin* (gentype _x_)
| Arc sine function.

| gentype *asinh* (gentype _x_)
| Inverse hyperbolic sine.

| gentype *asinpi* (gentype _x_)
| Compute *asin* (_x_) / {pi}.

| gentype *atan* (gentype _y_over_x_)
| Arc tangent function.

| gentype *atan2* (gentype _y_, gentype _x_)
| Arc tangent of _y_ / _x_.

| gentype *atanh* (gentype _x_)
| Hyperbolic arc tangent.

| gentype *atanpi* (gentype _x_)
| Compute *atan* (_x_) / {pi}.

| gentype *atan2pi* (gentype _y_, gentype _x_)
| Compute *atan2* (_y_, _x_) / {pi}.

| gentype *cbrt* (gentype _x_)
| Compute cube-root.

| gentype *ceil* (gentype _x_)
| Round to integral value using the round to positive infinity rounding
  mode.

| gentype *copysign* (gentype _x_, gentype _y_)
| Returns _x_ with its sign changed to match the sign of _y_.

| gentype *cos* (gentype _x_)
| Compute cosine.

| gentype *cosh* (gentype _x_)
| Compute hyperbolic consine.

| gentype *cospi* (gentype _x_)
| Compute *cos* ({pi} _x_).

| gentype *erfc* (gentype _x_)
| Complementary error function.

| gentype *erf* (gentype _x_)
| Error function encountered in integrating the normal distribution.

| gentype *exp* (gentype _x_)
| Compute the base- e exponential of _x_.

| gentype *exp2* (gentype _x_)
| Exponential base 2 function.

| gentype *exp10* (gentype _x_)
| Exponential base 10 function.

| gentype *expm1* (gentype _x_)
| Compute _e^x^_- 1.0.

| gentype *fabs* (gentype _x_)
| Compute absolute value of a floating-point number.

| gentype *fdim* (gentype _x_, gentype _y_)
| _x_ - _y_ if _x_ > _y_, +0 if x is less than or equal to y.

| gentype *floor* (gentype _x_)
| Round to integral value using the round to negative infinity rounding
  mode.

| gentype *fma* (gentype _a_, gentype _b_, gentype _c_)
| Returns the correctly rounded floating-point representation of the sum of
  _c_ with the infinitely precise product of _a_ and _b_.
  Rounding of intermediate products shall not occur.
  Edge case behavior is per the IEEE 754-2008 standard.

| gentype *fmax* (gentype _x_, gentype _y_) +
  gentype *fmax* (gentype _x_, double _y_)
| Returns _y_ if _x_ < _y_, otherwise it returns _x_.
  If one argument is a NaN, *fmax()* returns the other argument.
  If both arguments are NaNs, *fmax()* returns a NaN.

| gentype *fmin* (gentype _x_, gentype _y_) +
  gentype *fmin* (gentype _x_, double _y_)
| Returns _y_ if _y_ < _x_, otherwise it returns _x_.
  If one argument is a NaN, *fmin()* returns the other argument.
  If both arguments are NaNs, *fmin()* returns a NaN.

| gentype *fmod* (gentype _x_, gentype _y_)
| Modulus.
  Returns _x_ - _y_ * *trunc* (_x_/_y_) .

| gentype *fract* (gentype _x_, {global} gentype *_iptr_) +
  gentype *fract* (gentype _x_, {local} gentype *_iptr_) +
  gentype *fract* (gentype _x_, {private} gentype *_iptr_)
| Returns *fmin*( _x_ - *floor* (_x_), 0x1. fffffffffffffp-1 ).

  *floor*(x) is returned in _iptr_.

| double__n__ *frexp* (double__n x__, {global} int__n__ *exp) +
  double__n__ *frexp* (double__n x__, {local} int__n__ *exp) +
  double__n__ *frexp* (double__n x__, {private} int__n__ *exp) +
  double *frexp* (double _x_, {global} int *exp) +
  double *frexp* (double _x_, {local} int *exp) +
  double *frexp* (double _x_, {private} int *exp)
| Extract mantissa and exponent from _x_.
  For each component the mantissa returned is a float with magnitude in the
  interval [1/2, 1) or 0.
  Each component of _x_ equals mantissa returned * 2__^exp^__.

| gentype *hypot* (gentype _x_, gentype _y_)
| Compute the value of the square root of __x__2+ __y__2  without undue
  overflow or underflow.

| int__n__ *ilogb* (double__n__ _x_) +
  int *ilogb* (double _x_)
| Return the exponent as an integer value.

| double__n__ *ldexp* (double__n__ _x_, int__n__ _k_) +
  double__n__ *ldexp* (double__n__ _x_, int _k_) +
  double *ldexp* (double _x_, int _k_)
| Multiply _x_ by 2 to the power _k_.

| gentype **lgamma** (gentype _x_) +
  double__n__ **lgamma_r** (double__n__ _x_, {global} int__n__ *_signp_) +
  double__n__ **lgamma_r** (double__n__ _x_, {local} int__n__ *_signp_) +
  double__n__ **lgamma_r** (double__n__ _x_, {private} int__n__ *_signp_) +
  double **lgamma_r** (double _x_, {global} int *_signp_) +
  double **lgamma_r** (double _x_, {local} int *_signp_) +
  double **lgamma_r** (double _x_, {private} int *_signp_)
| Log gamma function.
  Returns the natural logarithm of the absolute value of the gamma function.
  The sign of the gamma function is returned in the _signp_ argument of
  *lgamma_r*.

| gentype *log* (gentype _x_)
| Compute natural logarithm.

| gentype *log2* (gentype _x_)
| Compute a base 2 logarithm.

| gentype *log10* (gentype _x_)
| Compute a base 10 logarithm.

| gentype *log1p* (gentype _x_)
| Compute log~e~(1.0 + _x_) .

| gentype *logb* (gentype _x_)
| Compute the exponent of _x_, which is the integral part of
  log__~r~__\|_x_\|.

| gentype *mad* (gentype _a_, gentype _b_, gentype _c_)
| *mad* computes _a_ * _b_ + _c_.
  The function may compute _a_ * _b_ + _c_ with reduced accuracy
  in the embedded profile.  See the SPIR-V OpenCL environment specification
  for details. On some hardware the mad instruction may provide better
  performance than expanded computation of _a_ * _b_ + _c_.

| gentype *maxmag* (gentype _x_, gentype _y_)
| Returns _x_ if \|_x_\| > \|_y_\|, _y_ if \|_y_\| > \|_x_\|, otherwise
  *fmax*(_x_, _y_).

| gentype *minmag* (gentype _x_, gentype _y_)
| Returns _x_ if \|_x_\| < \|_y_\|, _y_ if \|_y_\| < \|_x_\|, otherwise
  *fmin*(_x_, _y_).

| gentype *modf* (gentype _x_, {global} gentype *_iptr_) +
  gentype *modf* (gentype _x_, {local} gentype *_iptr_) +
  gentype *modf* (gentype _x_, {private} gentype *_iptr_)
| Decompose a floating-point number.
  The *modf* function breaks the argument _x_ into integral and fractional
  parts, each of which has the same sign as the argument.
  It stores the integral part in the object pointed to by _iptr_.

| double__n__ *nan* (ulong__n nancode__) +
  double *nan* (ulong _nancode_)
| Returns a quiet NaN.
  The _nancode_ may be placed in the significand of the resulting NaN.

| gentype *nextafter* (gentype _x_, gentype _y_)
| Computes the next representable double-precision floating-point value
  following _x_ in the direction of _y_.
  Thus, if _y_ is less than _x_, *nextafter*() returns the largest
  representable floating-point number less than _x_.

| gentype *pow* (gentype _x_, gentype _y_)
| Compute _x_ to the power _y_.

| double__n__ *pown* (double__n__ _x_, int__n__ _y_) +
  double *pown* (double _x_, int _y_)
| Compute _x_ to the power _y_, where _y_ is an integer.

| gentype *powr* (gentype _x_, gentype _y_)
| Compute _x_ to the power _y_, where _x_ is >= 0.

| gentype *remainder* (gentype _x_, gentype _y_)
| Compute the value _r_ such that _r_ = _x_ - _n_*_y_, where _n_ is the
  integer nearest the exact value of _x_/_y_.
  If there are two integers closest to _x_/_y_, _n_ shall be the even one.
  If _r_ is zero, it is given the same sign as _x_.

| double__n__ **remquo** (double__n__ _x_, double__n__ _y_, {global} int__n__ *_quo_) +
  double__n__ **remquo** (double__n__ _x_, double__n__ _y_, {local} int__n__ *_quo_) +
  double__n__ **remquo** (double__n__ _x_, double__n__ _y_, {private} int__n__ *_quo_) +
  double **remquo** (double _x_, double _y_, {global} int *_quo_) +
  double **remquo** (double _x_, double _y_, {local} int *_quo_) +
  double **remquo** (double _x_, double _y_, {private} int *_quo_)
| The *remquo* function computes the value r such that _r_ = _x_ - _k_*_y_,
  where _k_ is the integer nearest the exact value of _x_/_y_.
  If there are two integers closest to _x_/_y_, _k_ shall be the even one.
  If _r_ is zero, it is given the same sign as _x_.
  This is the same value that is returned by the *remainder* function.
  *remquo* also calculates the lower seven bits of the integral quotient
  _x_/_y_, and gives that value the same sign as _x_/_y_.
  It stores this signed value in the object pointed to by _quo_.

| gentype *rint* (gentype _x_)
| Round to integral value (using round to nearest even rounding mode) in
  floating-point format.
  Refer to section 7.1 for description of rounding modes.

| double__n__ *rootn* (double__n__ _x_, int__n__ _y_) +
  double__n__ *rootn* (double _x_, int _y_)
| Compute _x_ to the power 1/_y_.

| gentype *round* (gentype _x_)
| Return the integral value nearest to _x_ rounding halfway cases away from
  zero, regardless of the current rounding direction.

| gentype *rsqrt* (gentype _x_)
| Compute inverse square root.

| gentype *sin* (gentype _x_)
| Compute sine.

| gentype *sincos* (gentype _x_, {global} gentype *_cosval_) +
  gentype *sincos* (gentype _x_, {local} gentype *_cosval_) +
  gentype *sincos* (gentype _x_, {private} gentype *_cosval_)
| Compute sine and cosine of x.
  The computed sine is the return value and computed cosine is returned in
  _cosval_.

| gentype *sinh* (gentype _x_)
| Compute hyperbolic sine.

| gentype *sinpi* (gentype _x_)
| Compute *sin* ({pi} _x_).

| gentype *sqrt* (gentype _x_)
| Compute square root.

| gentype *tan* (gentype _x_)
| Compute tangent.

| gentype *tanh* (gentype _x_)
| Compute hyperbolic tangent.

| gentype *tanpi* (gentype _x_)
| Compute *tan* ({pi} _x_).

| gentype *tgamma* (gentype _x_)
| Compute the gamma function.

| gentype *trunc* (gentype _x_)
| Round to integral value using the round to zero rounding mode.
|====

In addition, the following symbolic constant will also be available:

*HUGE_VAL* - A positive double expression that evaluates to infinity.
Used as an error value returned by the built-in math functions.

The *FP_FAST_FMA* macro indicates whether the *fma()* family of
functions are fast compared with direct code for double precision
floating-point.
If defined, the *FP_FAST_FMA* macro shall indicate that the *fma()*
function generally executes about as fast as, or faster than, a multiply and
an add of *double* operands.

The macro names given in the following list must use the values specified.
These constant expressions are suitable for use in #if preprocessing
directives.

[source,c]
----
#define DBL_DIG             15
#define DBL_MANT_DIG        53
#define DBL_MAX_10_EXP      +308
#define DBL_MAX_EXP         +1024
#define DBL_MIN_10_EXP      -307
#define DBL_MIN_EXP         -1021
#define DBL_RADIX           2
#define DBL_MAX             0x1.fffffffffffffp1023
#define DBL_MIN             0x1.0p-1022
#define DBL_EPSILON         0x1.0p-52
----

The following table describes the built-in macro names given above in the
OpenCL C programming language and the corresponding macro names available to
the application.

[cols=",",options="header",]
|====
| *Macro in OpenCL Language* | *Macro for application*
| *DBL_DIG*                  | *CL_DBL_DIG*
| *DBL_MANT_DIG*             | *CL_DBL_MANT_DIG*
| *DBL_MAX_10_EXP*           | *CL_DBL_MAX_10_EXP*
| *DBL_MAX_EXP*              | *CL_DBL_MAX_EXP*
| *DBL_MIN_10_EXP*           | *CL_DBL_MIN_10_EXP*
| *DBL_MIN_EXP*              | *CL_DBL_MIN_EXP*
| *DBL_RADIX*                | *CL_DBL_RADIX*
| *DBL_MAX*                  | *CL_DBL_MAX*
| *DBL_MIN*                  | *CL_DBL_MIN*
| *DBL_EPSILSON*             | *CL_DBL_EPSILON*
|====

// TODO: DBL_RADIX / CL_DBL_RADIX?

The following constants are also available.
They are of type `double` and are accurate within the precision of the `double`
type.

[cols=",",options="header",]
|====
| *Constant*    | *Description*
| *M_E*         | Value of e
| *M_LOG2E*     | Value of log~2~e
| *M_LOG10E*    | Value of log~10~e
| *M_LN2*       | Value of log~e~2
| *M_LN10*      | Value of log~e~10
| *M_PI*        | Value of {pi}
| *M_PI_2*      | Value of {pi} / 2
| *M_PI_4*      | Value of {pi} / 4
| *M_1_PI*      | Value of 1 / {pi}
| *M_2_PI*      | Value of 2 / {pi}
| *M_2_SQRTPI*  | Value of 2 / {sqrt}{pi}
| *M_SQRT2*     | Value of {sqrt}2
| *M_SQRT1_2*   | Value of 1 / {sqrt}2
|====

[[cl_khr_fp64-common-functions]]
==== Common Functions

The built-in common functions defined in _table 6.12_ (also listed below)
are extended to include appropriate versions of functions that take `double`
and `double{2|3|4|8|16}` as arguments and return values.
gentype now also includes `double`, `double2`, `double3`, `double4`, `double8` and
`double16`.
These are described below.

.Double Precision Built-in Common Functions
[cols=",",options="header",]
|====
| *Function*
| *Description*

| gentype *clamp* ( +
  gentype _x_, gentype _minval_, gentype _maxval_)

  gentype *clamp* ( +
  gentype _x_, double _minval_, double _maxval_)
| Returns *min*(*max*(_x_, _minval_), _maxval_).

  Results are undefined if _minval_ > _maxval_.

| gentype *degrees* (gentype _radians_)
| Converts _radians_ to degrees, +
  i.e. (180 / {pi}) * _radians_.

| gentype *max* (gentype _x_, gentype _y_) +
  gentype *max* (gentype _x_, double _y_)
| Returns _y_ if _x_ < _y_, otherwise it returns _x_.
  If _x_ and _y_ are infinite or NaN, the return values are undefined.

| gentype *min* (gentype _x_, gentype _y_) +
  gentype *min* (gentype _x_, double _y_)
| Returns _y_ if _y_ < _x_, otherwise it returns _x_.
  If _x_ and _y_ are infinite or NaN, the return values are undefined.

| gentype *mix* (gentype _x_, gentype _y_, gentype _a_) +
  gentype *mix* (gentype _x_, gentype _y_, double _a_)
| Returns the linear blend of _x_ and _y_ implemented as:

  _x_ + (_y_ - _x)_ * _a_

  _a_ must be a value in the range 0.0 ... 1.0.
  If _a_ is not in the range 0.0 ... 1.0, the return values are undefined.

  Note: The double precision *mix* function can be implemented using contractions such as *mad* or *fma*.

| gentype *radians* (gentype _degrees_)
| Converts _degrees_ to radians, i.e. ({pi} / 180) * _degrees_.

| gentype *step* (gentype _edge_, gentype _x_) +
  gentype *step* (double _edge_, gentype _x_)
| Returns 0.0 if _x_ < _edge_, otherwise it returns 1.0.

| gentype *smoothstep* ( +
  gentype _edge0_, gentype _edge1_, gentype _x_) +

  gentype *smoothstep* ( +
  double _edge0_, double _edge1_, gentype _x_)
| Returns 0.0 if _x_ \<= _edge0_ and 1.0 if _x_ >= _edge1_ and performs
  smooth Hermite interpolation between 0 and 1 when _edge0_ < _x_ < _edge1_.
  This is useful in cases where you would want a threshold function with a
  smooth transition.

  This is equivalent to:

  gentype _t_; +
  _t_ = clamp ((_x_ - _edge0_) / (_edge1_ - _edge0_), 0, 1); +
  return _t_ * _t_ * (3 - 2 * _t_); +

  Results are undefined if _edge0_ >= _edge1_.

  Note: The double precision *smoothstep* function can be implemented using contractions such as *mad* or *fma*.

| gentype *sign* (gentype _x_)
| Returns 1.0 if _x_ > 0, -0.0 if _x_ = -0.0, +0.0 if _x_ = +0.0, or -1.0 if
  _x_ < 0.
  Returns 0.0 if _x_ is a NaN.

|====

[[cl_khr_fp64-geometric-functions]]
==== Geometric Functions

The built-in geometric functions defined in _table 6.13_ (also listed below)
are extended to include appropriate versions of functions that take `double`
and `double{2|3|4}` as arguments and return values.
gentype now also includes `double`, `double2`, `double3` and `double4`.
These are described below.

Note: The double precision geometric functions can be implemented using
contractions such as *mad* or *fma*.

._Double Precision Built-in Geometric Functions_
[cols=",",options="header",]
|====
| *Function*
| *Description*

| double4 *cross* (double4 _p0_, double4 _p1_) +
  double3 *cross* (double3 _p0_, double3 _p1_)
| Returns the cross product of _p0.xyz_ and _p1.xyz_.
  The _w_ component of the result will be 0.0.

| double *dot* (gentype _p0_, gentype _p1_)
| Compute the dot product of _p0_ and _p1_.

| double *distance* (gentype _p0_, gentype _p1_)
| Returns the distance between _p0_ and _p1_.
  This is calculated as *length*(_p0_ - _p1_).

| double *length* (gentype _p_)
| Return the length of vector x, i.e., +
  sqrt( __p.x__^2^ + __p.y__^2^ + ... )

| gentype *normalize* (gentype _p_)
| Returns a vector in the same direction as _p_ but with a length of 1.

|====

[[cl_khr_fp64-relational-functions]]
==== Relational Functions

The scalar and vector relational functions described in _table 6.14_ are
extended to include versions that take `double`, `double2`, `double3`, `double4`,
`double8` and `double16` as arguments.

The relational and equality operators (<, \<=, >, >=, !=, ==) can be used
with `doublen` vector types and shall produce a vector `longn` result as
described in _section 6.3_.

The functions *isequal*, *isnotequal*, *isgreater*, *isgreaterequal*,
*isless*, *islessequal*, *islessgreater*, *isfinite*, *isinf*, *isnan*,
*isnormal*, *isordered*, *isunordered* and *signbit* shall return a 0 if the
specified relation is _false_ and a 1 if the specified relation is true for
scalar argument types.
These functions shall return a 0 if the specified relation is _false_ and a
-1 (i.e. all bits set) if the specified relation is _true_ for vector
argument types.

The relational functions *isequal*, *isgreater*, *isgreaterequal*, *isless*,
*islessequal*, and *islessgreater* always return 0 if either argument is not
a number (NaN).
*isnotequal* returns 1 if one or both arguments are not a number (NaN) and
the argument type is a scalar and returns -1 if one or both arguments are
not a number (NaN) and the argument type is a vector.

The functions described in _table 6.14_ are extended to include the `doublen``
vector types.

._Double Precision Relational Functions_
[cols=",",options="header",]
|====
| *Function*
| *Description*

| int *isequal* (double _x_, double _y_) +
  long__n__ *isequal* (double__n x__, double__n y__)
| Returns the component-wise compare of _x_ == _y_.

| int *isnotequal* (double _x_, double _y_) +
  long__n__ *isnotequal* (double__n x__, double__n y__)
| Returns the component-wise compare of _x_ != _y_.

| int *isgreater* (double _x_, double _y_)
  long__n__ *isgreater* (double__n x__, double__n y__)
| Returns the component-wise compare of _x_ > _y_.

| int *isgreaterequal* (double _x_, double _y_) +
  long__n__ *isgreaterequal* (double__n x__, double__n y__)
| Returns the component-wise compare of _x_ >= _y_.

| int *isless* (double _x_, double _y_) +
  long__n__ *isless* (double__n x__, double__n y__)
| Returns the component-wise compare of _x_ < _y_.

| int *islessequal* (double _x_, double _y_) +
  long__n__ *islessequal* (double__n x__, double__n y__)
| Returns the component-wise compare of _x_ \<= _y_.

| int *islessgreater* (double _x_, double _y_) +
  long__n__ *islessgreater* (double__n x__, double__n y__)
| Returns the component-wise compare of (_x_ < _y_) \|\| (_x_ > _y_) .

| |

| int *isfinite* (double) +
  long__n__ *isfinite* (double__n__)
| Test for finite value.

| int *isinf* (double) +
  long__n__ *isinf* (double__n__)
| Test for infinity value (positive or negative) .

| int *isnan* (double) +
  long__n__ *isnan* (double__n__)
| Test for a NaN.

| int *isnormal* (double) +
  long__n__ *isnormal* (double__n__)
| Test for a normal value.

| int *isordered* (double _x_, double _y_) +
  long__n__ *isordered* (double__n x__, double__n y__)
| Test if arguments are ordered.
  *isordered*() takes arguments _x_ and _y_, and returns the result
  *isequal*(_x_, _x_) && *isequal*(_y_, _y_).

| int *isunordered* (double _x_, double _y_) +
  long__n__ *isunordered* (double__n x__, double__n y__)
| Test if arguments are unordered.
  *isunordered*() takes arguments _x_ and _y_, returning non-zero if _x_ or
  _y_ is a NaN, and zero otherwise.

| int *signbit* (double) +
  long__n__ *signbit* (double__n__)
| Test for sign bit.
  The scalar version of the function returns a 1 if the sign bit in the double
  is set else returns 0.
  The vector version of the function returns the following for each
  component in double__n__: -1 (i.e all bits set) if the sign bit in the double
  is set else returns 0.

| |

| double__n__ *bitselect* (double__n a__, double__n b__, double__n c__)
| Each bit of the result is the corresponding bit of _a_ if the
  corresponding bit of _c_ is 0.
  Otherwise it is the corresponding bit of _b_.

| double__n__ *select* (double__n a__, double__n b__, long__n c__) +
  double__n__ *select* (double__n a__, double__n b__, ulong__n c__)
| For each component, +
  _result[i]_ = if MSB of _c[i]_ is set ? _b[i]_ : _a[i]_. +

|====

[[cl_khr_fp64-vector-data-load-and-store-functions]]
==== Vector Data Load and Store Functions

The vector data load (*vload__n__*) and store (*vstore__n__*) functions
described in _table 6.13_ (also listed below) are extended to include
versions that read from or write to double scalar or vector values.
The generic type `gentype` is extended to include `double`.
The generic type `gentypen` is extended to include `double2`, `double3`,
`double4`, `double8` and `double16`.
The *vstore_half*, **vstore_half__n __**and **vstorea_half__n __**
functions are extended to allow a double precision scalar or vector
value to be written to memory as half values.

Note: *vload3* reads (_x_,_y_,_z_) components from address
`(_p_ + (_offset_ * 3))` into a 3-component vector.
*vstore3*, and *vstore_half3* write (_x_,_y_,_z_) components from a
3-component vector to address `(_p_ + (_offset_ * 3))`.
In addition, *vloada_half3* reads (_x_,_y_,_z_) components from address
`(_p_ + (_offset_ * 4))` into a 3-component vector and *vstorea_half3*
writes (_x_,_y_,_z_) components from a 3-component vector to address
`(_p_ + (_offset_ * 4))`.
Whether *vloada_half3* and *vstorea_half3* read/write padding data
between the third vector element and the next alignment boundary is
implementation defined.
*vloada_* and *vstoreaa_* variants are provided to access data that is
aligned to the size of the vector, and are intended to enable performance
on hardware that can take advantage of the increased alignment.

._Double Precision Vector Data Load and Store Functions_
[cols=",",options="header",]
|====
| *Function*
| *Description*

| gentype__n__ **vload__n__**(size_t _offset_, const {global} gentype *_p_)

  gentype__n__ **vload__n__**(size_t _offset_, const {local} gentype *_p_)

  gentype__n__ **vload__n__**(size_t _offset_, const {constant} gentype *_p_)

  gentype__n__ **vload__n__**(size_t _offset_, const {private} gentype *_p_)
| Return sizeof (gentype__n__) bytes of data read from address
  (_p_ + (_offset * n_)).
  The read address computed as (_p_ + (_offset * n_)) must be 8-bit aligned
  if gentype is char, uchar; 16-bit aligned if gentype is short, ushort;
  32-bit aligned if gentype is int, uint, float; 64-bit aligned if
  gentype is long, ulong or double.

| void **vstore__n__**(gentype__n__ _data_, size_t _offset_, {global} gentype *_p_)

  void **vstore__n__**(gentype__n__ _data_, size_t _offset_, {local} gentype *_p_)
  
  void **vstore__n__**(gentype__n__ _data_, size_t _offset_, {private} gentype *_p_)
| Write sizeof (gentype__n__) bytes given by _data_ to address
  (_p_ + (_offset * n_)).
  The address computed as (_p_ + (_offset * n_)) must be 8-bit aligned
  if gentype is char, uchar; 16-bit aligned if gentype is short, ushort;
  32-bit aligned if gentype is int, uint, float; 64-bit aligned
  if gentype is long, ulong or double.

| void **vstore_half**(double _data_, size_t _offset_, {global} half *_p_) +
  void **vstore_half{rte}**(double _data_, size_t _offset_, {global} half *_p_) +
  void **vstore_half{rtz}**(double _data_, size_t _offset_, {global} half *_p_) +
  void **vstore_half{rtp}**(double _data_, size_t _offset_, {global} half *_p_) +
  void **vstore_half{rtn}**(double _data_, size_t _offset_, {global} half *_p_) +

  void **vstore_half**(double _data_, size_t _offset_, {local} half *_p_) +
  void **vstore_half{rte}**(double _data_, size_t _offset_, {local} half *_p_) +
  void **vstore_half{rtz}**(double _data_, size_t _offset_, {local} half *_p_) +
  void **vstore_half{rtp}**(double _data_, size_t _offset_, {local} half *_p_) +
  void **vstore_half{rtn}**(double _data_, size_t _offset_, {local} half *_p_) +

  void **vstore_half**(double _data_, size_t _offset_, {private} half *_p_) +
  void **vstore_half{rte}**(double _data_, size_t _offset_, {private} half *_p_) +
  void **vstore_half{rtz}**(double _data_, size_t _offset_, {private} half *_p_) +
  void **vstore_half{rtp}**(double _data_, size_t _offset_, {private} half *_p_) +
  void **vstore_half{rtn}**(double _data_, size_t _offset_, {private} half *_p_)
| The double value given by _data_ is first converted to a half value
  using the appropriate rounding mode.
  The half value is then written to the address computed as
  (_p_ + _offset_).
  The address computed as (_p_ + _offset_) must be 16-bit aligned.

  *vstore_half* uses the current rounding mode.
  The default current rounding mode is round to nearest even.

| void **vstore_half__n__**(double__n__ _data_, size_t _offset_, {global} half *_p_) +
  void **vstore_half__n__{rte}**(double__n__ _data_, size_t _offset_, {global} half *_p_) +
  void **vstore_half__n__{rtz}**(double__n__ _data_, size_t _offset_, {global} half *_p_) +
  void **vstore_half__n__{rtp}**(double__n__ _data_, size_t _offset_, {global} half *_p_) +
  void **vstore_half__n__{rtn}**(double__n__ _data_, size_t _offset_, {global} half *_p_) +

  void **vstore_half__n__**(double__n__ _data_, size_t _offset_, {local} half *_p_) +
  void **vstore_half__n__{rte}**(double__n__ _data_, size_t _offset_, {local} half *_p_) +
  void **vstore_half__n__{rtz}**(double__n__ _data_, size_t _offset_, {local} half *_p_) +
  void **vstore_half__n__{rtp}**(double__n__ _data_, size_t _offset_, {local} half *_p_) +
  void **vstore_half__n__{rtn}**(double__n__ _data_, size_t _offset_, {local} half *_p_) +

  void **vstore_half__n__**(double__n__ _data_, size_t _offset_, {private} half *_p_) +
  void **vstore_half__n__{rte}**(double__n__ _data_, size_t _offset_, {private} half *_p_) +
  void **vstore_half__n__{rtz}**(double__n__ _data_, size_t _offset_, {private} half *_p_) +
  void **vstore_half__n__{rtp}**(double__n__ _data_, size_t _offset_, {private} half *_p_) +
  void **vstore_half__n__{rtn}**(double__n__ _data_, size_t _offset_, {private} half *_p_)
| The double__n__ value given by _data_ is converted to a half__n__ value
  using the appropriate rounding mode.
  The half__n __value is then written to the address computed as
  (_p_ + (_offset * n_)).
  The address computed as (_p_ + (_offset * n_)) must be 16-bit
  aligned.

  **vstore_half__n __**uses the current rounding mode.
  The default current rounding mode is round to nearest even.

| void **vstorea_half__n__**(double__n__ _data_, size_t _offset_, {global} half *_p_) +
  void **vstorea_half__n__{rte}**(double__n__ _data_, size_t _offset_, {global} half *_p_) +
  void **vstorea_half__n__{rtz}**(double__n__ _data_, size_t _offset_, {global} half *_p_) +
  void **vstorea_half__n__{rtp}**(double__n__ _data_, size_t _offset_, {global} half *_p_) +
  void **vstorea_half__n__{rtn}**(double__n__ _data_, size_t _offset_, {global} half *_p_) +

  void **vstorea_half__n__**(double__n__ _data_, size_t _offset_, {local} half *_p_) +
  void **vstorea_half__n__{rte}**(double__n__ _data_, size_t _offset_, {local} half *_p_) +
  void **vstorea_half__n__{rtz}**(double__n__ _data_, size_t _offset_, {local} half *_p_) +
  void **vstorea_half__n__{rtp}**(double__n__ _data_, size_t _offset_, {local} half *_p_) +
  void **vstorea_half__n__{rtn}**(double__n__ _data_, size_t _offset_, {local} half *_p_) +

  void **vstorea_half__n__**(double__n__ _data_, size_t _offset_, {private} half *_p_) +
  void **vstorea_half__n__{rte}**(double__n__ _data_, size_t _offset_, {private} half *_p_) +
  void **vstorea_half__n__{rtz}**(double__n__ _data_, size_t _offset_, {private} half *_p_) +
  void **vstorea_half__n__{rtp}**(double__n__ _data_, size_t _offset_, {private} half *_p_) +
  void **vstorea_half__n__{rtn}**(double__n__ _data_, size_t _offset_, {private} half *_p_)
| The double__n__ value is converted to a half__n__ value
  using the appropriate rounding mode.

  For n = 1, 2, 4, 8 or 16, the half__n__ value is written to the
  address computed as
  (_p_ + (_offset * n_)).
  The address computed as (_p_ + (_offset * n_)) must be aligned to
  sizeof (half__n__) bytes.

  For n = 3, the half__3__ value is written to the address computed as
  (_p_ + (_offset * 4_)).
  The address computed as (_p_ + (_offset * 4_)) must be aligned to
  sizeof (half) * 4 bytes.

  **vstorea_half__n__** uses the current rounding mode.
  The default current rounding mode is round to nearest even.
|====

[[cl_khr_fp64-async-copies-from-global-to-local-memory-local-to-global-memory-and-prefetch]]
==== Async Copies from Global to Local Memory, Local to Global Memory, and Prefetch

The OpenCL C programming language implements the following functions that
provide asynchronous copies between global and local memory and a prefetch
from global memory.

The generic type gentype is extended to include `double`, `double2`, `double3`,
`double4`, `double8` and `double16`.

._Double Precision Built-in Async Copy and Prefetch Functions_
[cols=",",options="header",]
|====
| *Function*
| *Description*

| event_t **async_work_group_copy** ( +
  {local} gentype *_dst_, +
  const {global} gentype *_src_, +
  size_t _num_gentypes_, event_t _event_)

  event_t **async_work_group_copy** ( +
  {global} gentype _*dst_, +
  const {local} gentype *_src_, +
  size_t _num_gentypes_, event_t _event_)
| Perform an async copy of _num_gentypes_ gentype elements from _src_ to
  _dst_.
  The async copy is performed by all work-items in a work-group and this
  built-in function must therefore be encountered by all work-items in a
  work-group executing the kernel with the same argument values; otherwise
  the results are undefined.

  Returns an event object that can be used by *wait_group_events* to wait
  for the async copy to finish.
  The _event_ argument can also be used to associate the
  *async_work_group_copy* with a previous async copy allowing an event to be
  shared by multiple async copies; otherwise _event_ should be zero.

  If _event_ argument is not zero, the event object supplied in _event_
  argument will be returned.

  This function does not perform any implicit synchronization of source data
  such as using a *barrier* before performing the copy.

| |

| event_t **async_work_group_strided_copy** ( +
  {local} gentype _*dst_, +
  const {global} gentype *_src_, +
  size_t _num_gentypes_, +
  size_t _src_stride_, event_t _event_)

  event_t **async_work_group_strided_copy** ( +
  {global} gentype _*dst_, +
  const {local} gentype *_src_, +
  size_t _num_gentypes_, +
  size_t _dst_stride_, event_t _event_)
| Perform an async gather of _num_gentypes_ gentype elements from _src_ to
  _dst_.
  The _src_stride_ is the stride in elements for each gentype element read
  from _src_.
  The async gather is performed by all work-items in a work-group and this
  built-in function must therefore be encountered by all work-items in a
  work-group executing the kernel with the same argument values; otherwise
  the results are undefined.

  Returns an event object that can be used by *wait_group_events* to wait
  for the async copy to finish.
  The _event_ argument can also be used to associate the
  *async_work_group_strided_copy* with a previous async copy allowing an
  event to be shared by multiple async copies; otherwise _event_ should be
  zero.

  If _event_ argument is not zero, the event object supplied in _event_
  argument will be returned.

  This function does not perform any implicit synchronization of source data
  such as using a *barrier* before performing the copy.

  The behavior of *async_work_group_strided_copy* is undefined if
  _src_stride_ or _dst_stride_ is 0, or if the _src_stride_ or _dst_stride_
  values cause the _src_ or _dst_ pointers to exceed the upper bounds of the
  address space during the copy.

| |

| void *wait_group_events* ( +
  int _num_events_, event_t *_event_list_)
| Wait for events that identify the *async_work_group_copy* operations to
  complete.
  The event objects specified in _event_list_ will be released after the
  wait is performed.

  This function must be encountered by all work-items in a work-group
  executing the kernel with the same _num_events_ and event objects
  specified in _event_list_; otherwise the results are undefined.

| void *prefetch* ( +
  const {global} gentype *__p__, size_t _num_gentypes_)
| Prefetch _num_gentypes_ * sizeof(gentype) bytes into the global cache.
  The prefetch instruction is applied to a work-item in a work-group and
  does not affect the functional behavior of the kernel.

|====

[[cl_khr_fp64-ieee754-compliance]]
==== IEEE754 Compliance

The following table entry describes the additions to _table 4.3,_ which
allows applications to query the configuration information using
*clGetDeviceInfo* for an OpenCL device that supports double precision
floating-point.

[cols="1,1,2",options="header",]
|====
| *Op-code*
| *Return Type*
| *Description*

| *CL_DEVICE_DOUBLE_FP_CONFIG*
| cl_device_fp_config
| Describes double precision floating-point capability of the OpenCL device.
  This is a bit-field that describes one or more of the following values:

  CL_FP_DENORM -- denorms are supported

  CL_FP_INF_NAN -- INF and NaNs are supported

  CL_FP_ROUND_TO_NEAREST -- round to nearest even rounding mode supported

  CL_FP_ROUND_TO_ZERO -- round to zero rounding mode supported

  CL_FP_ROUND_TO_INF -- round to positive and negative infinity rounding
  modes supported

  CL_FP_FMA -- IEEE754-2008 fused multiply-add is supported

  CL_FP_SOFT_FLOAT -- Basic floating-point operations (such as addition,
  subtraction, multiplication) are implemented in software.

  The required minimum double precision floating-point capability as
  implemented by this extension is:

  CL_FP_FMA \| +
  CL_FP_ROUND_TO_NEAREST \| +
  CL_FP_ROUND_TO_ZERO \| +
  CL_FP_ROUND_TO_INF \| +
  CL_FP_INF_NAN \| +
  CL_FP_DENORM.

|====

IEEE754 fused multiply-add, denorms, INF and NaNs are required to be
supported for double precision floating-point numbers and operations
on double precision floating-point numbers.

[[cl_khr_fp64-relative-error-as-ulps]]
==== Relative Error as ULPs

In this section we discuss the maximum relative error defined as _ulp_
(units in the last place).

Addition, subtraction, multiplication, fused multiply-add and conversion
between integer and a floating-point format are IEEE 754 compliant and
are therefore correctly rounded using round-to-nearest even rounding mode.

The following table describes the minimum accuracy of double precision
floating-point arithmetic operations given as ULP values.
0 ULP is used for math functions that do not require rounding.
The reference value used to compute the ULP value of an arithmetic operation
is the infinitely precise result.

._ULP Values for Double Precision Floating-Point Arithmetic Operations_
[cols=",",options="header",]
|====
| *Function*
| *Min Accuracy*

| *_x_ + _y_*
| Correctly rounded

| *_x_ - _y_*
| Correctly rounded

| *_x_ * _y_*
| Correctly rounded

| *1.0 / _x_*
| Correctly rounded

| *_x_ / _y_*
| Correctly rounded

| |

| *acos*
| \<= 4 ulp

| *acosh*
| \<= 4 ulp

| *acospi*
| \<= 5 ulp

| *asin*
| \<= 4 ulp

| *asinh*
| \<= 4 ulp

| *asinpi*
| \<= 5 ulp

| *atan*
| \<= 5 ulp

| *atanh*
| \<= 5 ulp

| *atanpi*
| \<= 5 ulp

| *atan2*
| \<= 6 ulp

| *atan2pi*
| \<= 6 ulp

| *cbrt*
| \<= 2 ulp

| *ceil*
| Correctly rounded

| *copysign*
| 0 ulp

| *cos*
| \<= 4 ulp

| *cosh*
| \<= 4 ulp

| *cospi*
| \<= 4 ulp

| *erfc*
| \<= 16 ulp

| *erf*
| \<= 16 ulp

| *exp*
| \<= 3 ulp

| *exp2*
| \<= 3 ulp

| *exp10*
| \<= 3 ulp

| *expm1*
| \<= 3 ulp

| *fabs*
| 0 ulp

| *fdim*
| Correctly rounded

| *floor*
| Correctly rounded

| *fma*
| Correctly rounded

| *fmax*
| 0 ulp

| *fmin*
| 0 ulp

| *fmod*
| 0 ulp

| *fract*
| Correctly rounded

| *frexp*
| 0 ulp

| *hypot*
| \<= 4 ulp

| *ilogb*
| 0 ulp

| *ldexp*
| Correctly rounded

| *log*
| \<= 3 ulp

| *log2*
| \<= 3 ulp

| *log10*
| \<= 3 ulp

| *log1p*
| \<= 2 ulp

| *logb*
| 0 ulp

| *mad*
| Implementation-defined

| *maxmag*
| 0 ulp

| *minmag*
| 0 ulp

| *modf*
| 0 ulp

| *nan*
| 0 ulp

| *nextafter*
| 0 ulp

| *pow(x, y)*
| \<= 16 ulp

| *pown(x, y)*
| \<= 16 ulp

| *powr(x, y)*
| \<= 16 ulp

| *remainder*
| 0 ulp

| *remquo*
| 0 ulp for the remainder, at least the lower 7 bits of the integral quotient

| *rint*
| Correctly rounded

| *rootn*
| \<= 16 ulp

| *round*
| Correctly rounded

| *rsqrt*
| \<= 2 ulp

| *sin*
| \<= 4 ulp

| *sincos*
| \<= 4 ulp for sine and cosine values

| *sinh*
| \<= 4 ulp

| *sinpi*
| \<= 4 ulp

| *sqrt*
| Correctly rounded

| *tan*
| \<= 5 ulp

| *tanh*
| \<= 5 ulp

| *tanpi*
| \<= 6 ulp

| *tgamma*
| \<= 16 ulp

| *trunc*
| Correctly rounded

|====
